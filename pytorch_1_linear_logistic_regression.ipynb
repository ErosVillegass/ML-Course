{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "XGElCAKxmBxQ",
        "vEX2A715Kj10",
        "snzq88EReDA_",
        "atJQZ75qKtTl"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ErosVillegass/ML-Course/blob/main/pytorch_1_linear_logistic_regression.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introducción"
      ],
      "metadata": {
        "id": "XGElCAKxmBxQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "2kXv2PAX7P-d",
        "outputId": "e7ab4b0e-c08e-4e88-9138-575251003ef7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([0.0597, 0.1799, 0.5045])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.13.1+cu116'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import torch\n",
        "x = torch.rand(3)\n",
        "print(x)\n",
        "torch.cuda.is_available()\n",
        "torch.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensors"
      ],
      "metadata": {
        "id": "RUe4qPmZ8YSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.empty(3) # create a empty tensor\n",
        "print(x)\n",
        "x = torch.empty(2,2) # create a empty tensor\n",
        "print(x)\n",
        "x = torch.zeros(2,3) # tensor of zeros\n",
        "print(x)\n",
        "x = torch.ones(2,2, dtype=torch.float16) # tensor of floats\n",
        "print(x, x.size())\n",
        "x = torch.tensor([2, 3])\n",
        "print(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmh60fHE8Z5B",
        "outputId": "f9b487cc-ec32-4998-cdb3-440bc8b2141f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1.4451e-35, 0.0000e+00, 3.3631e-44])\n",
            "tensor([[1.4450e-35, 0.0000e+00],\n",
            "        [3.3631e-44, 0.0000e+00]])\n",
            "tensor([[0., 0., 0.],\n",
            "        [0., 0., 0.]])\n",
            "tensor([[1., 1.],\n",
            "        [1., 1.]], dtype=torch.float16) torch.Size([2, 2])\n",
            "tensor([2, 3])\n",
            "tensor([[0.9484, 1.0900],\n",
            "        [1.4821, 1.6179]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# operations with tensors\n",
        "x = torch.rand(2,2)\n",
        "y = torch.rand(2,2)\n",
        "print(x, y)\n",
        "z = x + y\n",
        "print(z)\n",
        "\n",
        "y.add_(x) # modifica y y le agrega x\n",
        "print(y)\n",
        "\n",
        "z = x * y\n",
        "print(z)\n",
        "z = torch.mul(x, y) # torch.div, torch.add, torch.sub\n",
        "print(z)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lJ3i9Ee_IiP",
        "outputId": "de3b07c7-e17e-4d22-b910-ccb527edfacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.4351, 0.4462],\n",
            "        [0.7706, 0.5783]]) tensor([[0.1186, 0.7497],\n",
            "        [0.3971, 0.2931]])\n",
            "tensor([[0.5538, 1.1959],\n",
            "        [1.1677, 0.8714]])\n",
            "tensor([[0.5538, 1.1959],\n",
            "        [1.1677, 0.8714]])\n",
            "tensor([[0.2410, 0.5336],\n",
            "        [0.8999, 0.5039]])\n",
            "tensor([[0.2410, 0.5336],\n",
            "        [0.8999, 0.5039]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Slicing and reshape"
      ],
      "metadata": {
        "id": "chv7mMyLBODY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# slicing\n",
        "import torch\n",
        "\n",
        "x = torch.rand(5, 3)\n",
        "print(x[1:3, :])\n",
        "\n",
        "print(x[1,2].item()) # para extraer un elemento\n",
        "print(x.view(15)) # reshape to 1x15\n",
        "print(x.view(3, 5)) # reshape to 3x5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N9oVQf15AK0W",
        "outputId": "b10d3407-0957-4417-9a21-6a53d3c03737"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.0426, 0.9370, 0.5943],\n",
            "        [0.9736, 0.9646, 0.6275]])\n",
            "0.5943241715431213\n",
            "tensor([0.7340, 0.4004, 0.0248, 0.0426, 0.9370, 0.5943, 0.9736, 0.9646, 0.6275,\n",
            "        0.6849, 0.4495, 0.1598, 0.3819, 0.8652, 0.5290])\n",
            "tensor([[0.7340, 0.4004, 0.0248, 0.0426, 0.9370],\n",
            "        [0.5943, 0.9736, 0.9646, 0.6275, 0.6849],\n",
            "        [0.4495, 0.1598, 0.3819, 0.8652, 0.5290]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tensor and numpy are linked"
      ],
      "metadata": {
        "id": "WGPzFACDB8BE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "a = torch.ones(5) # a is a tensor in CPU\n",
        "#print(a)\n",
        "b = a.numpy()     # b is a numpy matrix in CPU, a and b are linked because both are on CPU\n",
        "print(b)\n",
        "\n",
        "# if we modify a or b, the other is modifed\n",
        "a.add_(1)\n",
        "print(a)\n",
        "print(b)\n",
        "\n",
        "# numpy to torch\n",
        "c = np.ones(5)\n",
        "d = torch.from_numpy(c)\n",
        "c += 1\n",
        "print(c)\n",
        "print(d)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TIzfIyftBVqT",
        "outputId": "b3e7ce00-1951-41bb-af76-ad1efb4d84a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., 1., 1., 1., 1.])\n",
            "[1. 1. 1. 1. 1.]\n",
            "tensor([2., 2., 2., 2., 2.])\n",
            "[2. 2. 2. 2. 2.]\n",
            "[2. 2. 2. 2. 2.]\n",
            "tensor([2., 2., 2., 2., 2.], dtype=torch.float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Escificamos el device de cuda"
      ],
      "metadata": {
        "id": "kznE2l0bDMhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "  device = torch.device(\"cuda\")\n",
        "  x = torch.ones(2, 2, device=device)  # x is in GPU\n",
        "  y = torch.ones(2, 2) # y is in CPU\n",
        "  y = y.to(device) # movemos a GPU\n",
        "  z = x + y # it performs in GPU\n",
        "  z = z.to(\"cpu\") # regresamos z al cpu\n"
      ],
      "metadata": {
        "id": "aivx2DHMDQ97"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradients"
      ],
      "metadata": {
        "id": "vEX2A715Kj10"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "PyTorch tiene la opción de calcular las gradientes (derivadas parciales en el algoritmo de backpropagation). <br>\n",
        "\n",
        "Por ejemplo para la función: $f = z(y(x))$, <br>\n",
        " donde: <br>\n",
        " $z = \\frac{1}{n} \\sum_{i = 1}^{n} (y_i)$ <br>\n",
        " $ y = x²$ <br>\n",
        " $z =\\frac{1}{n} \\sum_{i = 1}^{n} (x_i^2)$ <br>\n",
        "\n",
        "Entonces, la derivada parcial usando la regla de la cadena: $ \\frac{ \\partial{z}}{\\partial{x}} = \\frac{ \\partial{z}}{\\partial{y}} \\frac{ \\partial{y}}{\\partial{x}}$ \\\\\n",
        "\n",
        "Para $i = 1$ y $x=[ 0.5, 0.25, 0.125, 0.1 ]$ \\\\\n",
        "$\\frac{ \\partial{z}}{\\partial{y_1}} = \\frac{1}{n} \\left[ \\frac{\\partial{y_1}}{\\partial{y_1}} + \\frac{\\partial{y_2}}{\\partial{y_1}} +\\frac{\\partial{y_3}}{\\partial{y_1}} +\\frac{\\partial{y_4}}{\\partial{y_1}} \\right]$ \\\\\n",
        "\n",
        "Como estoy derivando respecto a $y_1$, entonces $y_2, y_3, y_4$ son constantes y serán ceros al derivar.\n",
        "\n",
        "$\\frac{ \\partial{z}}{\\partial{y_1}} = \\frac{1}{n} = \\frac{1}{4}$ \\\\\n",
        "\n",
        "Luego, la deriva parcial de $y$ respecto a $x_1$: \\\\\n",
        "\n",
        "$\\frac{ \\partial{y}}{\\partial{x_1}} = 2x$ \\\\\n",
        "\n",
        "Finalmente:\n",
        "\n",
        "$\\frac{ \\partial{z}}{\\partial{x_1}} = \\frac{x}{2}$ \\\\\n",
        "\n",
        "PyTorch, tiene un forma de calcular estas derivadas.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "x_o2_-BGKqJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "x = torch.tensor( [0.5, 0.25, 0.125, 0.1], requires_grad = True )\n",
        "\n",
        "# feed forward\n",
        "y = x**2\n",
        "z = y.mean()\n",
        "\n",
        "print(z)\n",
        "\n",
        "# backporpagation\n",
        "z.backward()\n",
        "print(x.grad) # aqui tenemos las gradientes para x_1, x_2, x_3 y x_4\n",
        "print(x/2)    # es lo mismo a lo que calculmos antes dz/dx = x/2\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o36tuL5FO8ST",
        "outputId": "8d99b6d4-a193-4ac4-db47-af74e4b6f66a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor(0.0845, grad_fn=<MeanBackward0>)\n",
            "tensor([0.2500, 0.1250, 0.0625, 0.0500])\n",
            "tensor([0.2500, 0.1250, 0.0625, 0.0500], grad_fn=<DivBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para quitar las gradientes de un tensor"
      ],
      "metadata": {
        "id": "B7WKQVZhKojL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "x = torch.randn(3, requires_grad=True)\n",
        "print(x)\n",
        "\n",
        "#option 1\n",
        "x.requires_grad_(False)\n",
        "print(x)\n",
        "\n",
        "#option 2\n",
        "x.detach()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cW4SxSnKPMEh",
        "outputId": "9a3b0990-39fd-4a3d-b4d9-4d456d09772e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.0166,  1.5275,  0.9359], requires_grad=True)\n",
            "tensor([-0.0166,  1.5275,  0.9359])\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([-0.0166,  1.5275,  0.9359])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión lineal con Numpy"
      ],
      "metadata": {
        "id": "q2m1gJsXmaiP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresion lineal con numpy. En este caso es una versión simple: $h(x) = wx$"
      ],
      "metadata": {
        "id": "y1dTfwM4pPAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# f = w * x\n",
        "X = np.array( [1, 2, 3, 4], dtype=np.float32 )\n",
        "Y = np.array( [2, 4, 6, 8], dtype=np.float32 )\n",
        "\n",
        "# h = theta_0 * x  solo usamos un parametro\n",
        "w = 0.0\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss\n",
        "def loss(y, y_pred):\n",
        "  return ((y - y_pred)**2).mean()/2\n",
        "\n",
        "# gradientes\n",
        "# MSE = 1/2m * (w*x - y)**2\n",
        "# dJ/dw = 1/m  (w*x - y)*x\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.02\n",
        "n_iters = 10\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # predciton = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w -= learning_rate * dw\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:0.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdJIA19JmsmA",
        "outputId": "ea533771-0be4-4422-860b-32c3926cb382"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 1.200, loss = 15.000\n",
            "epoch 2: w = 1.680, loss = 2.400\n",
            "epoch 3: w = 1.872, loss = 0.384\n",
            "epoch 4: w = 1.949, loss = 0.061\n",
            "epoch 5: w = 1.980, loss = 0.010\n",
            "epoch 6: w = 1.992, loss = 0.002\n",
            "epoch 7: w = 1.997, loss = 0.000\n",
            "epoch 8: w = 1.999, loss = 0.000\n",
            "epoch 9: w = 1.999, loss = 0.000\n",
            "epoch 10: w = 2.000, loss = 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresion lineal con numpy. En este caso con dos parametros: $h(x) = w_0 + w_1x$"
      ],
      "metadata": {
        "id": "4Z2ZuBpHqJJC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "X = np.array( [1, 2, 3, 4], dtype=np.float32 )\n",
        "Y = np.array( [3, 5, 7, 9], dtype=np.float32 )\n",
        "\n",
        "# h = w0 + w1 * x\n",
        "w0 = np.random.rand()\n",
        "w1 = np.random.rand()\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w0 + w1 * x\n",
        "\n",
        "# loss\n",
        "def loss(y, y_pred):\n",
        "  return ((y - y_pred)**2).mean()/2\n",
        "\n",
        "# gradientes\n",
        "# MSE = 1/2m * (w0 + w1*x - y)**2\n",
        "# dJ/dw0 = 1/m (w0 + w1*x - y)\n",
        "# dJ/dw1 = 1/m (w0 + w1*x - y)*x\n",
        "def gradient(x, y, y_pred):\n",
        "  return (y_pred - y).mean(), np.dot(x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.01\n",
        "n_iters = 30\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # predciton = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  dw0, dw1 = gradient(X, Y, y_pred)\n",
        "\n",
        "  # update weights\n",
        "  w0 -= learning_rate * dw0\n",
        "  w1 -= learning_rate * dw1\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w0 = {w0:.3f}, w1 = {w1:.3f}, loss = {l:0.3f}')\n",
        "\n",
        "y_pred = forward(X)\n",
        "print(Y)\n",
        "print(y_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "anAh07rbqHMB",
        "outputId": "7b47febc-6f6c-47ba-d398-b44850ed89ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 4.132\n",
            "epoch 1: w0 = 0.340, w1 = 1.206, loss = 8.106\n",
            "epoch 2: w0 = 0.367, w1 = 1.510, loss = 3.892\n",
            "epoch 3: w0 = 0.385, w1 = 1.720, loss = 1.876\n",
            "epoch 4: w0 = 0.399, w1 = 1.866, loss = 0.912\n",
            "epoch 5: w0 = 0.408, w1 = 1.966, loss = 0.450\n",
            "epoch 6: w0 = 0.415, w1 = 2.036, loss = 0.230\n",
            "epoch 7: w0 = 0.420, w1 = 2.083, loss = 0.124\n",
            "epoch 8: w0 = 0.423, w1 = 2.116, loss = 0.073\n",
            "epoch 9: w0 = 0.426, w1 = 2.139, loss = 0.049\n",
            "epoch 10: w0 = 0.428, w1 = 2.155, loss = 0.038\n",
            "epoch 11: w0 = 0.430, w1 = 2.166, loss = 0.032\n",
            "epoch 12: w0 = 0.432, w1 = 2.173, loss = 0.029\n",
            "epoch 13: w0 = 0.433, w1 = 2.178, loss = 0.028\n",
            "epoch 14: w0 = 0.434, w1 = 2.181, loss = 0.027\n",
            "epoch 15: w0 = 0.436, w1 = 2.183, loss = 0.027\n",
            "epoch 16: w0 = 0.437, w1 = 2.185, loss = 0.027\n",
            "epoch 17: w0 = 0.438, w1 = 2.186, loss = 0.026\n",
            "epoch 18: w0 = 0.439, w1 = 2.186, loss = 0.026\n",
            "epoch 19: w0 = 0.440, w1 = 2.186, loss = 0.026\n",
            "epoch 20: w0 = 0.441, w1 = 2.187, loss = 0.026\n",
            "epoch 21: w0 = 0.441, w1 = 2.187, loss = 0.026\n",
            "epoch 22: w0 = 0.442, w1 = 2.186, loss = 0.026\n",
            "epoch 23: w0 = 0.443, w1 = 2.186, loss = 0.026\n",
            "epoch 24: w0 = 0.444, w1 = 2.186, loss = 0.026\n",
            "epoch 25: w0 = 0.445, w1 = 2.186, loss = 0.026\n",
            "epoch 26: w0 = 0.446, w1 = 2.186, loss = 0.026\n",
            "epoch 27: w0 = 0.447, w1 = 2.185, loss = 0.026\n",
            "epoch 28: w0 = 0.448, w1 = 2.185, loss = 0.025\n",
            "epoch 29: w0 = 0.449, w1 = 2.185, loss = 0.025\n",
            "epoch 30: w0 = 0.450, w1 = 2.184, loss = 0.025\n",
            "[3. 5. 7. 9.]\n",
            "[2.6340487 4.818485  7.002921  9.187358 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Linear regression Sklearn"
      ],
      "metadata": {
        "id": "pwQfbxZJ8Y-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
        "# y = 1 * x_0 + 2 * x_1 + 3\n",
        "y = np.dot(X, np.array([1, 2])) + 3\n",
        "\n",
        "reg = LinearRegression().fit(X, y)\n",
        "print(reg.score(X, y))\n",
        "print(reg.coef_)\n",
        "print(reg.intercept_)\n",
        "reg.predict(np.array([[3, 5]]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGdti3QG8YG-",
        "outputId": "029d23b1-d258-4260-a5ca-6e9511f37060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.0\n",
            "[1. 2.]\n",
            "3.0000000000000018\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([16.])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Yl90Gbtv78rM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dzv4UuBztrmj",
        "outputId": "8e2f6f6b-5486-4a13-d0be-8b847a431dc1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4.738"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión lineal con PyTorch"
      ],
      "metadata": {
        "id": "snzq88EReDA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "# f = w * x\n",
        "X = torch.tensor( [1, 2, 3, 4], dtype=torch.float32 )\n",
        "Y = torch.tensor( [2, 4, 6, 8], dtype=torch.float32 )\n",
        "\n",
        "# h = theta_0 * x  solo usamos un parametro\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss\n",
        "def loss(y, y_pred):\n",
        "  return ((y - y_pred)**2).mean()/2\n",
        "\n",
        "# gradientes\n",
        "# MSE = 1/2m * (w*x - y)**2\n",
        "# dJ/dw = 1/m  (w*x - y)*x\n",
        "def gradient(x, y, y_pred):\n",
        "  return np.dot(x, y_pred - y).mean()\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# training\n",
        "learning_rate = 0.02\n",
        "n_iters = 40\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # predciton = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  #dw = gradient(X, Y, y_pred)\n",
        "  l.backward()\n",
        "\n",
        "  with torch.no_grad(): # no grad xq, no queremos que esta operación no se considere en las gradientes\n",
        "    # update weights\n",
        "    w -= learning_rate * w.grad\n",
        "\n",
        "  w.grad.zero_() # renicializamos las gradientes para que no se acumulen\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:0.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sG6MKn-vBWq_",
        "outputId": "3077a36f-814e-42f1-c8c2-e7f94c8ba5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.300, loss = 15.000\n",
            "epoch 2: w = 0.555, loss = 10.837\n",
            "epoch 3: w = 0.772, loss = 7.830\n",
            "epoch 4: w = 0.956, loss = 5.657\n",
            "epoch 5: w = 1.113, loss = 4.087\n",
            "epoch 6: w = 1.246, loss = 2.953\n",
            "epoch 7: w = 1.359, loss = 2.134\n",
            "epoch 8: w = 1.455, loss = 1.542\n",
            "epoch 9: w = 1.537, loss = 1.114\n",
            "epoch 10: w = 1.606, loss = 0.805\n",
            "epoch 11: w = 1.665, loss = 0.581\n",
            "epoch 12: w = 1.716, loss = 0.420\n",
            "epoch 13: w = 1.758, loss = 0.303\n",
            "epoch 14: w = 1.794, loss = 0.219\n",
            "epoch 15: w = 1.825, loss = 0.158\n",
            "epoch 16: w = 1.851, loss = 0.114\n",
            "epoch 17: w = 1.874, loss = 0.083\n",
            "epoch 18: w = 1.893, loss = 0.060\n",
            "epoch 19: w = 1.909, loss = 0.043\n",
            "epoch 20: w = 1.922, loss = 0.031\n",
            "epoch 21: w = 1.934, loss = 0.023\n",
            "epoch 22: w = 1.944, loss = 0.016\n",
            "epoch 23: w = 1.952, loss = 0.012\n",
            "epoch 24: w = 1.960, loss = 0.008\n",
            "epoch 25: w = 1.966, loss = 0.006\n",
            "epoch 26: w = 1.971, loss = 0.004\n",
            "epoch 27: w = 1.975, loss = 0.003\n",
            "epoch 28: w = 1.979, loss = 0.002\n",
            "epoch 29: w = 1.982, loss = 0.002\n",
            "epoch 30: w = 1.985, loss = 0.001\n",
            "epoch 31: w = 1.987, loss = 0.001\n",
            "epoch 32: w = 1.989, loss = 0.001\n",
            "epoch 33: w = 1.991, loss = 0.000\n",
            "epoch 34: w = 1.992, loss = 0.000\n",
            "epoch 35: w = 1.993, loss = 0.000\n",
            "epoch 36: w = 1.994, loss = 0.000\n",
            "epoch 37: w = 1.995, loss = 0.000\n",
            "epoch 38: w = 1.996, loss = 0.000\n",
            "epoch 39: w = 1.996, loss = 0.000\n",
            "epoch 40: w = 1.997, loss = 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresión lineal utilizando un optimizer y loss de pytorch"
      ],
      "metadata": {
        "id": "IwgDiIM5Dwe2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "# f = w * x\n",
        "X = torch.tensor( [1, 2, 3, 4], dtype=torch.float32 )\n",
        "Y = torch.tensor( [2, 4, 6, 8], dtype=torch.float32 )\n",
        "\n",
        "# h = theta_0 * x  solo usamos un parametro\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
        "\n",
        "learning_rate = 0.02\n",
        "n_iters = 40\n",
        "\n",
        "# model prediction\n",
        "def forward(x):\n",
        "  return w * x\n",
        "\n",
        "# loss\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "print(f'Prediction before training: f(5) = {forward(5):.3f}')\n",
        "\n",
        "# training\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "  # predciton = forward pass\n",
        "  y_pred = forward(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  l.backward()\n",
        "\n",
        "  optimizer.step() # actualiza los parametros\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    print(f'epoch {epoch+1}: w = {w:.3f}, loss = {l:0.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hU9GbQGAD4V2",
        "outputId": "c60033de-245f-4903-c434-ba64ce3e197f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = 0.000\n",
            "epoch 1: w = 0.600, loss = 30.000\n",
            "epoch 2: w = 1.020, loss = 14.700\n",
            "epoch 3: w = 1.314, loss = 7.203\n",
            "epoch 4: w = 1.520, loss = 3.529\n",
            "epoch 5: w = 1.664, loss = 1.729\n",
            "epoch 6: w = 1.765, loss = 0.847\n",
            "epoch 7: w = 1.835, loss = 0.415\n",
            "epoch 8: w = 1.885, loss = 0.203\n",
            "epoch 9: w = 1.919, loss = 0.100\n",
            "epoch 10: w = 1.944, loss = 0.049\n",
            "epoch 11: w = 1.960, loss = 0.024\n",
            "epoch 12: w = 1.972, loss = 0.012\n",
            "epoch 13: w = 1.981, loss = 0.006\n",
            "epoch 14: w = 1.986, loss = 0.003\n",
            "epoch 15: w = 1.991, loss = 0.001\n",
            "epoch 16: w = 1.993, loss = 0.001\n",
            "epoch 17: w = 1.995, loss = 0.000\n",
            "epoch 18: w = 1.997, loss = 0.000\n",
            "epoch 19: w = 1.998, loss = 0.000\n",
            "epoch 20: w = 1.998, loss = 0.000\n",
            "epoch 21: w = 1.999, loss = 0.000\n",
            "epoch 22: w = 1.999, loss = 0.000\n",
            "epoch 23: w = 1.999, loss = 0.000\n",
            "epoch 24: w = 2.000, loss = 0.000\n",
            "epoch 25: w = 2.000, loss = 0.000\n",
            "epoch 26: w = 2.000, loss = 0.000\n",
            "epoch 27: w = 2.000, loss = 0.000\n",
            "epoch 28: w = 2.000, loss = 0.000\n",
            "epoch 29: w = 2.000, loss = 0.000\n",
            "epoch 30: w = 2.000, loss = 0.000\n",
            "epoch 31: w = 2.000, loss = 0.000\n",
            "epoch 32: w = 2.000, loss = 0.000\n",
            "epoch 33: w = 2.000, loss = 0.000\n",
            "epoch 34: w = 2.000, loss = 0.000\n",
            "epoch 35: w = 2.000, loss = 0.000\n",
            "epoch 36: w = 2.000, loss = 0.000\n",
            "epoch 37: w = 2.000, loss = 0.000\n",
            "epoch 38: w = 2.000, loss = 0.000\n",
            "epoch 39: w = 2.000, loss = 0.000\n",
            "epoch 40: w = 2.000, loss = 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Regresión lineal utilizando, loss, optimizer y model de PyTorch"
      ],
      "metadata": {
        "id": "rVFcvU6aFTEW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1) design model\n",
        "2) construct loss and optimizer\n",
        "3) training\n",
        "    - forward pass\n",
        "    - backward pass, compute gradients\n",
        "    - update parameters\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "X = torch.tensor( [[1], [2], [3], [4]], dtype=torch.float32 )\n",
        "Y = torch.tensor( [[2], [4], [6], [8]], dtype=torch.float32 )\n",
        "X_test = torch.tensor( [5], dtype=torch.float32 )\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "input_size = n_features\n",
        "output_size  = n_features\n",
        "\n",
        "# option 1 -> use Linear directly\n",
        "# model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# option 2 -> create a class\n",
        "class MyLinearRegression(nn.Module):\n",
        "  def __init__(self, input_file, output_file):\n",
        "    super(MyLinearRegression, self).__init__()\n",
        "\n",
        "    # layers\n",
        "    self.lin = nn.Linear(input_size, output_size)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.lin(x)\n",
        "\n",
        "model = MyLinearRegression(input_size, output_size)\n",
        "###################################################\n",
        "\n",
        "learning_rate = 0.02\n",
        "n_iters = 40\n",
        "\n",
        "print(f'Prediction before training: f(5) = {model(X_test).item():.3f}')\n",
        "\n",
        "# loss\n",
        "loss = nn.MSELoss()\n",
        "\n",
        "# optimizer\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training\n",
        "for epoch in range(n_iters):\n",
        "  # forward pass\n",
        "  y_pred = model(X)\n",
        "\n",
        "  # loss\n",
        "  l = loss(Y, y_pred)\n",
        "\n",
        "  # gradients\n",
        "  l.backward()\n",
        "\n",
        "  optimizer.step() # actualiza los parametros\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if epoch % 1 == 0:\n",
        "    [w, b] = model.parameters()\n",
        "    print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {l:0.3f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YayjEJxFV5R",
        "outputId": "84a863dd-556e-4f7d-cb78-9e7e7061f4d1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction before training: f(5) = -4.467\n",
            "epoch 1: w = 0.025, loss = 64.021\n",
            "epoch 2: w = 0.609, loss = 28.438\n",
            "epoch 3: w = 0.999, loss = 12.655\n",
            "epoch 4: w = 1.259, loss = 5.654\n",
            "epoch 5: w = 1.432, loss = 2.548\n",
            "epoch 6: w = 1.548, loss = 1.171\n",
            "epoch 7: w = 1.626, loss = 0.559\n",
            "epoch 8: w = 1.678, loss = 0.287\n",
            "epoch 9: w = 1.713, loss = 0.166\n",
            "epoch 10: w = 1.737, loss = 0.112\n",
            "epoch 11: w = 1.753, loss = 0.087\n",
            "epoch 12: w = 1.765, loss = 0.076\n",
            "epoch 13: w = 1.773, loss = 0.071\n",
            "epoch 14: w = 1.778, loss = 0.068\n",
            "epoch 15: w = 1.783, loss = 0.066\n",
            "epoch 16: w = 1.786, loss = 0.065\n",
            "epoch 17: w = 1.788, loss = 0.064\n",
            "epoch 18: w = 1.790, loss = 0.063\n",
            "epoch 19: w = 1.792, loss = 0.062\n",
            "epoch 20: w = 1.794, loss = 0.062\n",
            "epoch 21: w = 1.795, loss = 0.061\n",
            "epoch 22: w = 1.797, loss = 0.060\n",
            "epoch 23: w = 1.798, loss = 0.059\n",
            "epoch 24: w = 1.799, loss = 0.059\n",
            "epoch 25: w = 1.801, loss = 0.058\n",
            "epoch 26: w = 1.802, loss = 0.057\n",
            "epoch 27: w = 1.803, loss = 0.057\n",
            "epoch 28: w = 1.804, loss = 0.056\n",
            "epoch 29: w = 1.805, loss = 0.055\n",
            "epoch 30: w = 1.807, loss = 0.055\n",
            "epoch 31: w = 1.808, loss = 0.054\n",
            "epoch 32: w = 1.809, loss = 0.053\n",
            "epoch 33: w = 1.810, loss = 0.053\n",
            "epoch 34: w = 1.811, loss = 0.052\n",
            "epoch 35: w = 1.812, loss = 0.051\n",
            "epoch 36: w = 1.813, loss = 0.051\n",
            "epoch 37: w = 1.815, loss = 0.050\n",
            "epoch 38: w = 1.816, loss = 0.050\n",
            "epoch 39: w = 1.817, loss = 0.049\n",
            "epoch 40: w = 1.818, loss = 0.048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión lineal (completo)"
      ],
      "metadata": {
        "id": "atJQZ75qKtTl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1) design model\n",
        "2) construct loss and optimizer\n",
        "3) training\n",
        "    - forward pass\n",
        "    - backward pass, compute gradients\n",
        "    - update parameters\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# prepare datasets\n",
        "X_numpy, y_numpy = datasets.make_regression(n_samples=100, n_features=1, noise = 20, random_state=1)\n",
        "\n",
        "X = torch.from_numpy(X_numpy.astype(np.float32))\n",
        "y = torch.from_numpy(y_numpy.astype(np.float32))\n",
        "y = y.view(y.shape[0], 1)\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "# model\n",
        "input_size = n_features\n",
        "output_size = 1\n",
        "model = nn.Linear(input_size, output_size)\n",
        "\n",
        "# loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forwars pass\n",
        "  y_predicted = model(X)\n",
        "  loss = criterion(y_predicted, y)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss: {loss.item():.4f}')\n",
        "\n",
        "\n",
        "# plot\n",
        "predicted = model(X).detach().numpy()\n",
        "plt.plot(X_numpy, y_numpy, 'ro')\n",
        "plt.plot(X_numpy, predicted, 'b')\n",
        "plt.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6C478qDvKywm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "outputId": "7af9ffff-4220-4098-dd75-2810d6518b0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss: 4293.2505\n",
            "epoch: 20, loss: 3204.7151\n",
            "epoch: 30, loss: 2417.2241\n",
            "epoch: 40, loss: 1846.9069\n",
            "epoch: 50, loss: 1433.4591\n",
            "epoch: 60, loss: 1133.4550\n",
            "epoch: 70, loss: 915.5817\n",
            "epoch: 80, loss: 757.2305\n",
            "epoch: 90, loss: 642.0568\n",
            "epoch: 100, loss: 558.2316\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regresión logística con PyTorch"
      ],
      "metadata": {
        "id": "qGAqg2z3xzOz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "1) design model\n",
        "2) construct loss and optimizer\n",
        "3) training\n",
        "    - forward pass\n",
        "    - backward pass, compute gradients\n",
        "    - update parameters\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# prepare datasets\n",
        "bc = datasets.load_breast_cancer() # binary dataset\n",
        "X, y = bc.data, bc.target\n",
        "\n",
        "n_samples, n_features = X.shape\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 1234)\n",
        "\n",
        "# scale\n",
        "sc = StandardScaler()\n",
        "X_train = sc.fit_transform(X_train)\n",
        "X_test = sc.fit_transform(X_test)\n",
        "\n",
        "X_train = torch.from_numpy(X_train.astype(np.float32))\n",
        "X_test = torch.from_numpy(X_test.astype(np.float32))\n",
        "y_train = torch.from_numpy(y_train.astype(np.float32))\n",
        "y_test = torch.from_numpy(y_test.astype(np.float32))\n",
        "\n",
        "y_train = y_train.view(y_train.shape[0], 1)\n",
        "y_test = y_test.view(y_test.shape[0], 1)\n",
        "\n",
        "\n",
        "# model\n",
        "# f = wx + b, sigmoid at the end\n",
        "class LogisticRegression(nn.Module):\n",
        "  def __init__(self, n_input_features):\n",
        "    super(LogisticRegression, self).__init__()\n",
        "    self.linear = nn.Linear(n_input_features, 1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    y_predicted = torch.sigmoid(self.linear(x))\n",
        "    return y_predicted\n",
        "\n",
        "model = LogisticRegression(n_features)\n",
        "\n",
        "\n",
        "# loss and optimizer\n",
        "learning_rate = 0.01\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# training loop\n",
        "num_epochs = 100\n",
        "for epoch in range(num_epochs):\n",
        "  # forwars pass\n",
        "  y_predicted = model(X_train)\n",
        "  loss = criterion(y_predicted, y_train)\n",
        "\n",
        "  # backward pass\n",
        "  loss.backward()\n",
        "\n",
        "  # update\n",
        "  optimizer.step()\n",
        "\n",
        "  optimizer.zero_grad()\n",
        "\n",
        "  if (epoch+1) % 10 == 0:\n",
        "    print(f'epoch: {epoch+1}, loss: {loss.item():.4f}')\n",
        "\n",
        "# si nohacemos esto, y_predicted.round() será considerado parte del grafo para el compute de las gradientes\n",
        "with torch.no_grad():\n",
        "  y_predicted = model(X_test)\n",
        "  y_predicted_cls = y_predicted.round()\n",
        "  acc = y_predicted_cls.eq(y_test).sum() / float(y_test.shape[0])\n",
        "  print(f'accuracy: {acc:.4f}')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SX6C57fOx596",
        "outputId": "0084042d-bb97-4017-8ac3-1fe2b0e3085f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 10, loss: 0.4420\n",
            "epoch: 20, loss: 0.3927\n",
            "epoch: 30, loss: 0.3562\n",
            "epoch: 40, loss: 0.3279\n",
            "epoch: 50, loss: 0.3052\n",
            "epoch: 60, loss: 0.2865\n",
            "epoch: 70, loss: 0.2707\n",
            "epoch: 80, loss: 0.2572\n",
            "epoch: 90, loss: 0.2454\n",
            "epoch: 100, loss: 0.2351\n",
            "accuracy: 0.9123\n"
          ]
        }
      ]
    }
  ]
}